{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblue'>\n",
    "\n",
    "<font size = 5>\n",
    "    Use the movie review dataset for sentiment analysis<br><br>\n",
    "</font>\n",
    "</font>\n",
    "\n",
    "<font color = 'grey'>\n",
    "<font size = 4>\n",
    "    \n",
    "**Following examples are included in the processing:**\n",
    "\n",
    "- `Download` data set from Google Cloud Storage\n",
    "- `Load` dataset from csv file\n",
    "- `Preprocess` data\n",
    "- `Create` training and test dataset\n",
    "- `Tokenize` the sequences\n",
    "- `Explore` the tokenization\n",
    "- `Create` a neural network\n",
    "- `Train` model and make predictions\n",
    "- `Explore` model performance\n",
    "\n",
    "</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit, time\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Keras Embedding Layer<br>\n",
    "\n",
    "<font size = 3>\n",
    "Let us take these 2 sentences:\n",
    "    \n",
    "1. `Hello how are you doing`\n",
    "2. `Hello how are you feeling`\n",
    "    \n",
    "- We have 6 unique words in our vocabulary\n",
    "- Say we want to learn 2 weights for each of these words\n",
    "- Say we assign integers to the words we have, Hello = 0, how = 1, are = 2 ....\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2345)\n",
    "emModel = tf.keras.Sequential()\n",
    "emModel.add(tf.keras.layers.Embedding(input_dim = 6, output_dim = 2, input_length = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.expand_dims(np.array([0,1,2,3,4,5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata = emModel.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 3>\n",
    " \n",
    "`Note: ` These are initial weights for our embedding vector for each word\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the location to datasets folder\n",
    "currLoc = os.getcwd()\n",
    "print(f'current folder: {currLoc}')\n",
    "os.chdir('..')\n",
    "cwd = os.getcwd()\n",
    "print(f'current working directory: {cwd}')\n",
    "path = cwd + '/' + 'datasets'\n",
    "print(f'path: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset to download\n",
    "\n",
    "dnfile = \"https://storage.googleapis.com/courses-datasets/AI-ML-Toolkit/IMDBDataset.csv\"\n",
    "print(f'Cloud file location: {dnfile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the dataset from Google Cloud\n",
    "\n",
    "!wget {dnfile} -NP {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory to where code is\n",
    "\n",
    "os.chdir(currLoc)\n",
    "cwd = os.getcwd()\n",
    "print(f'current working directory: {cwd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50,000 movie reviews labeled as \"positive\" or \"negative\"\n",
    "df = pd.read_csv(\"../datasets/IMDBDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the sentiment (target variable) to numbers\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess function\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    Preprocess the input review to do the following:\n",
    "    <ol>\n",
    "        <li>Convert to lower case</li>\n",
    "        <li>Remove html tags if found</li>\n",
    "        <li>Remove puncutations</li>\n",
    "    </ol>\n",
    "</span>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def preprocessData(input_data):\n",
    "    lowercase = input_data.lower()\n",
    "    stripped_html = lowercase.replace('<br />', ' ')\n",
    "    retval = re.sub(r'[^\\w\\s]','', stripped_html)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the review column\n",
    "df['review'] = df['review'].map(preprocessData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example of taking top row for training and bottom for testing\n",
    "X_train = df[\"review\"].values[:25000]\n",
    "X_test = df[\"review\"].values[25000:]\n",
    "\n",
    "y_train = df[\"sentiment\"].values[:25000]\n",
    "y_test  = df[\"sentiment\"].values[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[\"review\"].values\n",
    "y = df[\"sentiment\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.50, \n",
    "                                                    random_state = 2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train[y_train == 0]), len(y_train[y_train == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test[y_test == 0]), len(y_test[y_test == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define hyper parameters that can be modified\n",
    "vocab_size = 20000\n",
    "embedding_dim = 32\n",
    "# If a sentence is shorter than max_length it will be padded, \n",
    "# longer sentences will be truncated\n",
    "max_length = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Out of Vocabulary token rather than throwing away unknown words\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token = \"<oov>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to numbers and pad for the neural network to use as input\n",
    "# if the sequence length is greater than max length then truncate it at the end\n",
    "# if the sequence length is less than max length then pad it at the end\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, \n",
    "                             padding = \"post\", truncating=\"post\")\n",
    "\n",
    "\n",
    "# tokenized using the word_index learned from the training data\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(testing_sequences, maxlen=max_length, \n",
    "                            padding = \"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverses keys: keys become the values, and values become the keys so that \n",
    "# we can look a word up (display padded as ?)\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "# This is what will be fed in\n",
    "print(decode_review(train_padded[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original text\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    Model Parameters:\n",
    "    <ol>\n",
    "        <li><b>Embedding: </b>\n",
    "            <ul>\n",
    "                <li>Embedding layer stores one vector per word</li>\n",
    "                <li>Converts sequences of word indices to sequences of vectors</li>\n",
    "                <li>These vectors are trainable</li>\n",
    "                <li>Once trained, words with similar meanings often have similar vectors</li>\n",
    "                <li>This approach is more efficient than using a dense layer with one hot encoding</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>GlobalAveragePooling1D: </b>\n",
    "            <ul>\n",
    "                <li>Returns a fixed length output vector for each example by averaging over the sequence dimension</li>\n",
    "                <li>Allows the model to handle input of variable length</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>Couple of Dense layers: </b>\n",
    "            <ul>\n",
    "                <li>Apply the dense layer with ReLU activation</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "    The last output layer use sigmoid to get probability of positive or negative sentiment\n",
    "</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create model\n",
    "model = tf.keras.Sequential([\n",
    "    # The Embedding layer is the key to text sentiment analysis\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "tf.random.set_seed(2345)\n",
    "model = tf.keras.Sequential([\n",
    "    # The Embedding layer is the key to text sentiment analysis\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu', name = 'FirstHidden'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(name = 'AvgPooling1'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'Output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Timer for measuring time taken to train the model\n",
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.random.set_seed(2345)\n",
    "history = model.fit(train_padded, y_train, epochs=3, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Stop timer and print training time\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "exectime = time.strftime(\"%M:%S\", time.gmtime(execution_time)) \n",
    "print(\"To train it took: {} mins\".format(exectime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = model.metrics_names\n",
    "metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string1, string2):\n",
    "    # 2 rows 1 column\n",
    "    plt.subplots(2, 1, sharex=False, sharey=False, figsize=(8,6))\n",
    "    # plot 1\n",
    "    plt.subplot(211)\n",
    "    plt.plot(history.history[string1])\n",
    "    plt.plot(history.history['val_'+string1])\n",
    "    plt.ylabel(string1)\n",
    "    plt.legend([string1, 'val_'+string1]);\n",
    "    \n",
    "    # plot 2\n",
    "    plt.subplot(212)\n",
    "    plt.plot(history.history[string2])\n",
    "    plt.plot(history.history['val_'+string2])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string2)\n",
    "    plt.legend([string2, 'val_'+string2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, metrics_names[0], metrics_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from the Embedding layer\n",
    "embeddings = model.layers[0]\n",
    "\n",
    "weights = embeddings.get_weights()[0]\n",
    "print(f\"Vocabulary size: {weights.shape[0]},  Embedding dimensions: {weights.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape is: \n",
    "# (the number of words in the corpus, the embedding dimensions)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_padded, y_test)\n",
    "\n",
    "print(\"Test accuracy: \", test_acc)\n",
    "print(\"Test loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a positive sample\n",
    "\n",
    "sample_text_to_predict = \\\n",
    "[\"The movie was great. The animation and the graphics was excellent. I would recommend this movie.\"]\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(sample_text_to_predict)\n",
    "pos_padded = pad_sequences(train_sequences, maxlen=max_length, padding = \"post\", \n",
    "                           truncating=\"post\")\n",
    "\n",
    "#  make prediction\n",
    "prediction = model.predict(pos_padded)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a negative sample\n",
    "\n",
    "sample_text_to_predict = \\\n",
    "[\"The movie was horrible. The animation and the graphics were worst. I would not recommend this movie.\"]\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(sample_text_to_predict)\n",
    "neg_padded = pad_sequences(train_sequences, maxlen=max_length, padding = \"post\", \n",
    "                           truncating=\"post\")\n",
    "\n",
    "#  make prediction\n",
    "prediction = model.predict(neg_padded)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# writing the vectors and their metadata out to file. \n",
    "# these 2 files ('vecs.tsv', 'meta.tsv') are used by the \n",
    "# TensorFlow projector (http://projector.tensorflow.org/)\n",
    "# to plot/visualize the vectors/embeddings in 3D\n",
    "\n",
    "# Output of the 16 values per word representation (embedding)\n",
    "#      out_v are the weights (embedding)\n",
    "#      out_m are the actual words associated with each embedding\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
